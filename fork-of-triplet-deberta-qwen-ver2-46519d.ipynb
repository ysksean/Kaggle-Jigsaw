{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"sourceType":"competition"},{"sourceId":4620664,"sourceType":"datasetVersion","datasetId":2663421},{"sourceId":12726948,"sourceType":"datasetVersion","datasetId":8044304},{"sourceId":12762469,"sourceType":"datasetVersion","datasetId":8067935},{"sourceId":259545323,"sourceType":"kernelVersion"},{"sourceId":271051632,"sourceType":"kernelVersion"},{"sourceId":124328,"sourceType":"modelInstanceVersion","modelInstanceId":104636,"modelId":128845},{"sourceId":124336,"sourceType":"modelInstanceVersion","modelInstanceId":104644,"modelId":128845},{"sourceId":171496,"sourceType":"modelInstanceVersion","modelInstanceId":145960,"modelId":164048},{"sourceId":171638,"sourceType":"modelInstanceVersion","modelInstanceId":146086,"modelId":164048},{"sourceId":426330,"sourceType":"modelInstanceVersion","modelInstanceId":347541,"modelId":368803},{"sourceId":605940,"sourceType":"modelInstanceVersion","modelInstanceId":454557,"modelId":470738}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### References\n\n*   [https://www.kaggle.com/code/abdmental01/jigsaw-mpnet-base-v2-inference-cv-0-876](https://www.kaggle.com/code/abdmental01/jigsaw-mpnet-base-v2-inference-cv-0-876)\n*   [https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo](https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo)\n*   [https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/](https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/)\n*   https://www.kaggle.com/code/mks2192/jigsaw-llama3-1-8b-instruct-training-one-epoch\n*   [https://www.kaggle.com/code/fuumin621/qwen2-5-lora-finetune-baseline-inference](https://www.kaggle.com/code/fuumin621/qwen2-5-lora-finetune-baseline-inference)\n*   https://www.kaggle.com/code/neibyr/30-min-just-use-semantic-search-qwen3-emb-0-6b\n*   https://www.kaggle.com/code/datafan07/jigsaw-speed-run-10-min-triplet-and-faiss\n*   https://www.kaggle.com/code/nahidhossainredom/deberta-v3-base-3-epochs-lb-0-906\n*   https://www.kaggle.com/code/wasupandceacar/jigsaw-pseudo-training-llama-3-2-3b-instruct","metadata":{}},{"cell_type":"code","source":"!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'trl==0.21.0' 'optimum==1.27.0' 'auto-gptq==0.7.1' 'bitsandbytes==0.46.1' 'logits-processor-zoo==0.2.1' 'vllm==0.10.0'\n!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'deepspeed==0.17.4' -q\n!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'triton==3.2.0'\n!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'clean-text'\n!uv pip install --system --no-index -U --no-deps --find-links='/kaggle/input/jigsaw-packages2/whls/' 'peft' 'accelerate' 'datasets'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T23:52:09.93709Z","iopub.execute_input":"2025-10-14T23:52:09.937336Z","iopub.status.idle":"2025-10-14T23:52:56.067341Z","shell.execute_reply.started":"2025-10-14T23:52:09.937312Z","shell.execute_reply":"2025-10-14T23:52:56.066644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile constants.py\n\nseed = 0\n\nbase_model_path = \"/kaggle/input/jigsaw-pretrain-public/pytorch/llama-3.2-3b-instruct/1\"\npretrain_lora_path = None\nlora_path = \"/kaggle/working/pseudo_lora\"\nuse_gptq = \"gptq\" in base_model_path\n\npositive = \"Yes\"\nnegative = \"No\"\njudge_words = \"Violation:\"\nsystem_prompt = '''You are given a comment from reddit and a rule. \nYour task is to classify whether the comment violates the rule. \nOnly respond Yes/No.'''\n\nfrac = 0.05\nuse_train = True\n\nimport kagglehub\n\ndeterministic = kagglehub.package_import('wasupandceacar/deterministic').deterministic\ndeterministic.init_all(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T23:52:56.069103Z","iopub.execute_input":"2025-10-14T23:52:56.06938Z","iopub.status.idle":"2025-10-14T23:52:56.075274Z","shell.execute_reply.started":"2025-10-14T23:52:56.069354Z","shell.execute_reply":"2025-10-14T23:52:56.074521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile utils.py\n\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset\nfrom constants import *\n\ndef build_prompt(row):\n    return f\"\"\"{system_prompt}\nSubreddit: r/{row[\"subreddit\"]}\nRule: {row[\"rule\"]}\nExamples:\n1) {row[\"positive_example\"]}\n{judge_words} Yes\n2) {row[\"negative_example\"]}\n{judge_words} No\nComment: {row[\"body\"]}\n{judge_words}\"\"\"\n\ndef get_df():\n    merge = list()\n    if use_train:\n        train_dataset = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/train.csv\")\n        train_df = train_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\",\n                                \"positive_example_1\", \"positive_example_2\", \n                                \"negative_example_1\", \"negative_example_2\"]].copy()\n        train_df[\"positive_example\"] = np.where(np.random.rand(len(train_df)) < 0.5, train_df[\"positive_example_1\"], train_df[\"positive_example_2\"])\n        train_df[\"negative_example\"] = np.where(np.random.rand(len(train_df)) < 0.5, train_df[\"negative_example_1\"], train_df[\"negative_example_2\"])\n        train_df.drop(columns=[\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"], inplace=True)\n        merge.append(train_df)\n    test_dataset = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/test.csv\")\n    test_dataset = test_dataset.groupby('rule', group_keys=False).apply(lambda x: x.sample(frac=frac, random_state=seed)).reset_index(drop=True)\n    print(f\"Select {len(test_dataset)} test data\")\n    for violation_type in [\"positive\", \"negative\"]:\n        for i in range(1, 3):\n            sub_dataset = test_dataset[[\"rule\", \"subreddit\", \"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"]].copy()\n            body_col = f\"{violation_type}_example_{i}\"\n            other_positive_col = f\"{violation_type}_example_{3-i}\"\n            sub_dataset[\"body\"] = sub_dataset[body_col]\n            sub_dataset[f\"{violation_type}_example\"] = sub_dataset[other_positive_col]\n            anti_violation_type = \"negative\" if violation_type == \"positive\" else \"positive\"\n            sub_dataset[f\"{anti_violation_type}_example\"] = np.where(np.random.rand(len(sub_dataset)) < 0.5, sub_dataset[f\"{anti_violation_type}_example_1\"], sub_dataset[f\"{anti_violation_type}_example_2\"])\n            sub_dataset[\"rule_violation\"] = 1 if violation_type == \"positive\" else 0\n            sub_dataset.drop(columns=[\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"], inplace=True)\n            merge.append(sub_dataset)\n    return pd.concat(merge, axis=0).drop_duplicates(ignore_index=True)\n\ndef build_dataset(df):\n    df[\"prompt\"] = df.apply(build_prompt, axis=1)\n    columns = [\"prompt\"]\n    if \"rule_violation\" in df:\n        df[\"completion\"] = df[\"rule_violation\"].map({\n            1: positive,\n            0: negative,})\n        columns.append(\"completion\")\n    dataset = Dataset.from_pandas(df[columns])\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T23:52:56.076194Z","iopub.execute_input":"2025-10-14T23:52:56.076453Z","iopub.status.idle":"2025-10-14T23:52:56.116965Z","shell.execute_reply.started":"2025-10-14T23:52:56.076428Z","shell.execute_reply":"2025-10-14T23:52:56.116383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train.py\n\nimport torch\nimport pandas as pd\nfrom trl import SFTTrainer, SFTConfig\nfrom peft import PeftModel, LoraConfig, get_peft_model\nfrom tqdm.auto import tqdm\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom transformers.utils import is_torch_bf16_gpu_available\n\nfrom utils import *\nfrom constants import *\n\ndef main():\n    train_dataset = build_dataset(get_df())\n    lora_config = LoraConfig(\n        r=64,\n        lora_alpha=128,\n        lora_dropout=0.1,\n        bias=\"none\",\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n        task_type=\"CAUSAL_LM\",\n    )\n    \n    training_args = SFTConfig(\n        num_train_epochs=1,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        optim=\"paged_adamw_8bit\",\n        learning_rate=1e-4,\n        weight_decay=0.01,\n        max_grad_norm=1.0,\n        lr_scheduler_type=\"cosine\",\n        warmup_ratio=0.03,\n        bf16=is_torch_bf16_gpu_available(),\n        fp16=not is_torch_bf16_gpu_available(),\n        dataloader_pin_memory=True,\n        gradient_checkpointing=True,\n        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n        save_strategy=\"no\",\n        report_to=\"none\",\n        completion_only_loss=True,\n        packing=False,\n        remove_unused_columns=False,\n    )\n\n    if use_gptq:\n        model = AutoModelForCausalLM.from_pretrained(\n            base_model_path,\n            device_map=\"balanced_low_0\",\n            trust_remote_code=True,\n            use_cache=False,\n        )\n    else:\n        model = AutoModelForCausalLM.from_pretrained(\n            base_model_path,\n            quantization_config=BitsAndBytesConfig(\n                load_in_4bit=True,     \n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch.bfloat16,\n                bnb_4bit_use_double_quant=True,\n            ),\n            device_map=\"balanced_low_0\",\n            trust_remote_code=True,\n            use_cache=False,\n        )\n    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n    tokenizer.pad_token = tokenizer.eos_token\n    if pretrain_lora_path:\n        model = PeftModel.from_pretrained(model, pretrain_lora_path)\n        model = model.merge_and_unload()\n\n    if len(train_dataset) > 0:\n        trainer = SFTTrainer(\n            model=model,\n            processing_class=tokenizer,\n            args=training_args,\n            train_dataset=train_dataset,\n            peft_config=lora_config,\n        )\n        trainer.train()\n        trainer.save_model(lora_path)\n    else:\n        peft_model = get_peft_model(model, lora_config)\n        peft_model.save_pretrained(lora_path)\n        tokenizer.save_pretrained(lora_path)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T23:52:56.11787Z","iopub.execute_input":"2025-10-14T23:52:56.118083Z","iopub.status.idle":"2025-10-14T23:52:56.131069Z","shell.execute_reply.started":"2025-10-14T23:52:56.118058Z","shell.execute_reply":"2025-10-14T23:52:56.130449Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile accelerate_config.yaml\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndeepspeed_config:\n  gradient_accumulation_steps: 4\n  gradient_clipping: 1.0\n  train_micro_batch_size_per_gpu: 4\n  \n  zero_stage: 2\n  offload_optimizer_device: none\n  offload_param_device: none\n  zero3_init_flag: false\n  \n  stage3_gather_16bit_weights_on_model_save: false\n  stage3_max_live_parameters: 1e8\n  stage3_max_reuse_distance: 1e8\n  stage3_prefetch_bucket_size: 5e7\n  stage3_param_persistence_threshold: 1e5\n  \n  zero_allow_untested_optimizer: true\n  zero_force_ds_cpu_optimizer: false\n  \n  # fp16:\n  #   enabled: true\n  #   loss_scale: 0\n  #   initial_scale_power: 16\n  #   loss_scale_window: 1000\n  #   hysteresis: 2\n  #   min_loss_scale: 1\n  bf16:\n    enabled: true\n  \ndistributed_type: DEEPSPEED\ndowncast_bf16: 'yes'\ndynamo_config:\n  dynamo_backend: INDUCTOR\n  dynamo_use_fullgraph: false\n  dynamo_use_dynamic: false\nenable_cpu_affinity: false\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile inference.py\n\nimport os\nos.environ[\"VLLM_USE_V1\"] = \"0\"\n\nimport random\nimport vllm\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\nfrom vllm.lora.request import LoRARequest\nfrom utils import build_dataset\nfrom constants import *\nimport multiprocessing as mp\n\ndef run_inference_on_device(df_slice):\n    llm = vllm.LLM(\n        base_model_path,\n        quantization=\"gptq\" if use_gptq else None,\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.98,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=2048,\n        disable_log_stats=True,\n        enable_prefix_caching=True,\n        enable_lora=True,\n        max_lora_rank=64,\n    )\n    tokenizer = llm.get_tokenizer()\n    outputs = llm.generate(\n        build_dataset(df_slice)[\"prompt\"],\n        vllm.SamplingParams(\n            skip_special_tokens=True,\n            max_tokens=1,\n            logits_processors=[MultipleChoiceLogitsProcessor(tokenizer, choices=[positive, negative])],\n            logprobs=2,\n        ),\n        use_tqdm=True,\n        lora_request=LoRARequest(\"lora1\", 1, lora_path)\n    )\n    log_probs = [{lp.decoded_token: np.exp(lp.logprob) for lp in out.outputs[0].logprobs[0].values()} for out in outputs]\n    predictions = pd.DataFrame(log_probs)[[positive, negative]]\n    predictions[\"row_id\"] = df_slice[\"row_id\"].values\n    return predictions\n\ndef worker(device_id, df_slice, return_dict):\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device_id)\n    print(f\"[Worker {device_id}] Running on GPU {device_id}, data size={len(df_slice)}\")\n    preds = run_inference_on_device(df_slice)\n    return_dict[device_id] = preds\n\ndef main():\n    test_df = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/test.csv\")\n    test_df[\"positive_example\"] = test_df.apply(lambda row: random.choice([row[\"positive_example_1\"], row[\"positive_example_2\"]]), axis=1)\n    test_df[\"negative_example\"] = test_df.apply(lambda row: random.choice([row[\"negative_example_1\"], row[\"negative_example_2\"]]), axis=1)\n    test_df = test_df.drop(columns=[\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"], errors=\"ignore\")\n\n    mid = len(test_df) // 2\n    df0 = test_df.iloc[:mid].reset_index(drop=True)\n    df1 = test_df.iloc[mid:].reset_index(drop=True)\n\n    manager = mp.Manager()\n    return_dict = manager.dict()\n    p0 = mp.Process(target=worker, args=(0, df0, return_dict))\n    p1 = mp.Process(target=worker, args=(1, df1, return_dict))\n    p0.start()\n    p1.start()\n    p0.join()\n    p1.join()\n\n    predictions = pd.concat([return_dict[0], return_dict[1]], ignore_index=True)\n    submission = predictions[[\"row_id\", positive]].rename(columns={positive: \"rule_violation\"})\n    submission.to_csv(\"/kaggle/working/submission_qwen.csv\", index=False)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T23:52:56.131962Z","iopub.execute_input":"2025-10-14T23:52:56.132201Z","iopub.status.idle":"2025-10-14T23:52:56.145474Z","shell.execute_reply.started":"2025-10-14T23:52:56.132181Z","shell.execute_reply":"2025-10-14T23:52:56.144808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!accelerate launch --config_file accelerate_config.yaml train.py\n!python inference.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T23:52:56.160126Z","iopub.execute_input":"2025-10-14T23:52:56.160344Z","iopub.status.idle":"2025-10-15T00:04:03.439893Z","shell.execute_reply.started":"2025-10-14T23:52:56.160328Z","shell.execute_reply":"2025-10-15T00:04:03.43914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T00:05:17.815603Z","iopub.execute_input":"2025-10-15T00:05:17.815882Z","iopub.status.idle":"2025-10-15T00:05:18.093109Z","shell.execute_reply.started":"2025-10-15T00:05:17.815839Z","shell.execute_reply":"2025-10-15T00:05:18.092526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile constants.py\nEMBDEDDING_MODEL_PATH = \"/kaggle/input/qwen-3-embedding/transformers/0.6b/1\"\nMODEL_OUTPUT_PATH = '/kaggle/input/qwen3-8b-embedding'\nDATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules\"\n\n# https://huggingface.co/Qwen/Qwen3-Embedding-0.6B/blob/main/config_sentence_transformers.json\nEMBEDDING_MODEL_QUERY = \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery:\"\n\nCLEAN_TEXT = True\nTOP_K = 2000\nBATCH_SIZE = 128","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T00:05:18.093806Z","iopub.execute_input":"2025-10-15T00:05:18.094315Z","iopub.status.idle":"2025-10-15T00:05:18.099499Z","shell.execute_reply.started":"2025-10-15T00:05:18.094287Z","shell.execute_reply":"2025-10-15T00:05:18.098614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile utils.py\nimport pandas as pd\nimport torch.distributed as dist\n\nfrom datasets import Dataset\nfrom cleantext import clean\nfrom tqdm.auto import tqdm\n\nfrom constants import CLEAN_TEXT\n\n\ndef build_prompt(row):\n    return f\"\"\"r/{row[\"subreddit\"]}\\nComment: {row[\"body\"]}\"\"\"\n\n\ndef cleaner(text):\n    return clean(\n        text,\n        fix_unicode=True,\n        to_ascii=True,\n        lower=False,\n        no_line_breaks=False,\n        no_urls=True,\n        no_emails=True,\n        no_phone_numbers=True,\n        no_numbers=False,\n        no_digits=False,\n        no_currency_symbols=False,\n        no_punct=False,\n        replace_with_url=\"<URL>\",\n        replace_with_email=\"<EMAIL>\",\n        replace_with_phone_number=\"<PHONE>\",\n        lang=\"en\",\n    )\n\n\n\ndef get_dataframe_to_train(data_path):\n    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n    test_dataset = pd.read_csv(f\"{data_path}/test.csv\").sample(frac=0.6, random_state=42).reset_index(drop=True)\n\n    flatten = []\n    flatten.append(train_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\"]])\n    \n    for violation_type in [\"positive\", \"negative\"]:\n        for i in range(1, 3):\n            sub_dataset = test_dataset[[f\"{violation_type}_example_{i}\", \"rule\", \"subreddit\"]].copy()\n            sub_dataset = sub_dataset.rename(columns={f\"{violation_type}_example_{i}\": \"body\"})\n            sub_dataset[\"rule_violation\"] = 1 if violation_type == \"positive\" else 0\n            flatten.append(sub_dataset)\n\n    dataframe = pd.concat(flatten, axis=0)    \n    dataframe = dataframe.drop_duplicates(ignore_index=True)\n    return dataframe\n\n\ndef prepare_dataframe(dataframe):\n    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n\n    \n    if CLEAN_TEXT:\n        tqdm.pandas(desc=\"cleaner\")\n        dataframe[\"prompt\"] = dataframe[\"prompt\"].progress_apply(cleaner)\n\n    if \"rule_violation\" in dataframe.columns:\n        dataframe[\"rule_violation\"] = dataframe[\"rule_violation\"].map(\n            {\n                1: 1,\n                0: -1,\n            }\n        )\n\n    return dataframe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T00:05:18.10031Z","iopub.execute_input":"2025-10-15T00:05:18.100709Z","iopub.status.idle":"2025-10-15T00:05:18.112892Z","shell.execute_reply.started":"2025-10-15T00:05:18.100685Z","shell.execute_reply":"2025-10-15T00:05:18.112314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile semantic.py\nimport pandas as pd\nfrom transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import semantic_search, dot_score\nfrom tqdm.auto import tqdm\nfrom peft import PeftModel, PeftConfig\n\n\nfrom utils import get_dataframe_to_train, prepare_dataframe\nfrom constants import DATA_PATH, EMBDEDDING_MODEL_PATH, EMBEDDING_MODEL_QUERY, TOP_K, BATCH_SIZE, MODEL_OUTPUT_PATH\n\n\n\ndef get_scores(test_dataframe):\n    corpus_dataframe = get_dataframe_to_train(DATA_PATH)\n    corpus_dataframe = prepare_dataframe(corpus_dataframe)\n    \n    # Load base model\n    model = AutoModelForCausalLM.from_pretrained(EMBDEDDING_MODEL_PATH)\n    tokenizer = AutoTokenizer.from_pretrained(EMBDEDDING_MODEL_PATH)\n    \n    # Load adapter configuration and model\n    adapter_config = PeftConfig.from_pretrained(MODEL_OUTPUT_PATH)\n    lora_model = PeftModel.from_pretrained(model, MODEL_OUTPUT_PATH, config=adapter_config)\n    merged_model = lora_model.merge_and_unload()\n    tokenizer.save_pretrained(\"Qwen3Emb_Finetuned\")\n    merged_model.save_pretrained(\"Qwen3Emb_Finetuned\")\n\n    # 4. Tạo lại SentenceTransformer từ encoder đã merge\n    embedding_model = SentenceTransformer(model_name_or_path=\"Qwen3Emb_Finetuned\", device=\"cuda\")\n\n    print('Done loading model!')\n\n    result = []\n    for rule in tqdm(test_dataframe[\"rule\"].unique(), desc=f\"Generate scores for each rule\"):\n        test_dataframe_part = test_dataframe.query(\"rule == @rule\").reset_index(drop=True)\n        corpus_dataframe_part = corpus_dataframe.query(\"rule == @rule\").reset_index(drop=True)\n        corpus_dataframe_part = corpus_dataframe_part.reset_index(names=\"row_id\")\n        \n        query_embeddings = embedding_model.encode(\n            sentences=test_dataframe_part[\"prompt\"].tolist(),\n            prompt=EMBEDDING_MODEL_QUERY,\n            batch_size=BATCH_SIZE,\n            show_progress_bar=True,\n            convert_to_tensor=True,\n            device=\"cuda\",\n            normalize_embeddings=True,\n        )\n        document_embeddings = embedding_model.encode(\n            sentences=corpus_dataframe_part[\"prompt\"].tolist(),\n            batch_size=BATCH_SIZE,\n            show_progress_bar=True,\n            convert_to_tensor=True,\n            device=\"cuda\",\n            normalize_embeddings=True,\n        )\n        test_dataframe_part[\"semantic\"] = semantic_search(\n            query_embeddings,\n            document_embeddings,\n            top_k=TOP_K,\n            score_function=dot_score,\n        )\n        def get_score(semantic):\n            semantic = pd.DataFrame(semantic)\n            semantic = semantic.merge(\n                corpus_dataframe_part[[\"row_id\", \"rule_violation\"]],\n                how=\"left\",\n                left_on=\"corpus_id\",\n                right_on=\"row_id\",\n            )\n            semantic[\"score\"] = semantic[\"score\"]*semantic[\"rule_violation\"]\n            return semantic[\"score\"].sum()\n            \n        tqdm.pandas(desc=f\"Add label for {rule=}\")\n        test_dataframe_part[\"rule_violation\"] = test_dataframe_part[\"semantic\"].progress_apply(get_score)\n        result.append(test_dataframe_part[[\"row_id\", \"rule_violation\"]].copy())\n        \n    submission = pd.concat(result, axis=0)\n    return submission\n\n\ndef generate_submission():\n    test_dataframe = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n    test_dataframe = prepare_dataframe(test_dataframe)\n    \n    submission = get_scores(test_dataframe)\n    submission = test_dataframe[[\"row_id\"]].merge(submission, on=\"row_id\", how=\"left\")\n    submission.to_csv(\"submission_qwen3.csv\", index=False)\n\n\nif __name__ == \"__main__\":\n    generate_submission()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python semantic.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T00:05:18.113535Z","iopub.execute_input":"2025-10-15T00:05:18.113721Z","iopub.status.idle":"2025-10-15T00:05:18.351435Z","shell.execute_reply.started":"2025-10-15T00:05:18.113705Z","shell.execute_reply":"2025-10-15T00:05:18.350682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile triplet.py\n#!/usr/bin/env python3\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nimport pandas as pd\nimport numpy as np\nimport random\nfrom datasets import Dataset\nfrom sentence_transformers import (\n    SentenceTransformer,\n    SentenceTransformerTrainer,\n    SentenceTransformerTrainingArguments,\n    models\n)\nfrom sentence_transformers.losses import TripletLoss\nfrom sklearn.metrics.pairwise import cosine_similarity  # (원본 유지)\nimport re\nfrom urllib.parse import urlparse\nimport faiss  # (원본 유지)\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Advanced clustering\nfrom sklearn.cluster import AgglomerativeClustering\nfrom umap import UMAP\n\n# -----------------------------\n# Helpers (원본 동일)\n# -----------------------------\ndef cleaner(text):\n    \"\"\"Replace URLs with format: <url>: (domain/important-path)\"\"\"\n    if not text:\n        return text\n    url_pattern = r'https?://[^\\s<>\"{}|\\\\^`\\[\\]]+'\n    def replace_url(match):\n        url = match.group(0)\n        try:\n            parsed = urlparse(url)\n            domain = parsed.netloc.lower()\n            if domain.startswith('www.'):\n                domain = domain[4:]\n            path_parts = [part for part in parsed.path.split('/') if part]\n            if path_parts:\n                important_path = '/'.join(path_parts[:2])\n                return f\"<url>: ({domain}/{important_path})\"\n            else:\n                return f\"<url>: ({domain})\"\n        except:\n            return \"<url>: (unknown)\"\n    return re.sub(url_pattern, replace_url, str(text))\n\n\ndef load_test_data():\n    \"\"\"Load test data.\"\"\"\n    print(\"Loading test data...\")\n    test_df = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/test.csv')\n    print(f\"Loaded {len(test_df)} test examples\")\n    print(f\"Unique rules: {test_df['rule'].nunique()}\")\n    return test_df\n\n\ndef collect_all_texts(test_df):\n    \"\"\"Collect all unique texts from test set.\"\"\"\n    print(\"\\nCollecting all texts for embedding...\")\n    all_texts = set()\n    for body in test_df['body']:\n        if pd.notna(body):\n            all_texts.add(cleaner(str(body)))\n    example_cols = ['positive_example_1', 'positive_example_2',\n                    'negative_example_1', 'negative_example_2']\n    for col in example_cols:\n        for example in test_df[col]:\n            if pd.notna(example):\n                all_texts.add(cleaner(str(example)))\n    all_texts = list(all_texts)\n    print(f\"Collected {len(all_texts)} unique texts\")\n    return all_texts\n\n\ndef generate_embeddings(texts, model, batch_size=64):\n    \"\"\"Generate BGE embeddings for all texts.\"\"\"\n    print(f\"Generating embeddings for {len(texts)} texts...\")\n    embeddings = model.encode(\n        sentences=texts,\n        batch_size=batch_size,\n        show_progress_bar=True,\n        convert_to_tensor=False,\n        normalize_embeddings=True\n    )\n    return embeddings\n\n\ndef create_test_triplet_dataset(test_df, augmentation_factor=2, random_seed=42, subsample_fraction=1.0):\n    \"\"\"Create triplet dataset from test data: anchor=rule, positive=positive_example, negative=negative_example.\"\"\"\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    anchors, positives, negatives = [], [], []\n    print(\"Creating rule-aligned triplets from test data...\")\n    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing test rows\"):\n        rule = cleaner(str(row['rule']))\n        pos_examples = []\n        neg_examples = []\n        for neg_col in ['negative_example_1', 'negative_example_2']:  # Compliant → triplet positive\n            if pd.notna(row[neg_col]):\n                pos_examples.append(cleaner(str(row[neg_col])))\n        for pos_col in ['positive_example_1', 'positive_example_2']:  # Violating → triplet negative\n            if pd.notna(row[pos_col]):\n                neg_examples.append(cleaner(str(row[pos_col])))\n        for pos_ex in pos_examples:\n            for neg_ex in neg_examples:\n                anchors.append(rule)\n                positives.append(pos_ex)\n                negatives.append(neg_ex)\n\n    if augmentation_factor > 0:\n        print(f\"Adding {augmentation_factor}x augmentation...\")\n        rule_positives = {}\n        rule_negatives = {}\n        for rule in test_df['rule'].unique():\n            rule_df = test_df[test_df['rule'] == rule]\n            pos_pool, neg_pool = [], []\n            for _, row in rule_df.iterrows():\n                for neg_col in ['negative_example_1', 'negative_example_2']:\n                    if pd.notna(row[neg_col]):\n                        pos_pool.append(cleaner(str(row[neg_col])))\n                for pos_col in ['positive_example_1', 'positive_example_2']:\n                    if pd.notna(row[pos_col]):\n                        neg_pool.append(cleaner(str(row[pos_col])))\n            rule_positives[rule] = list(set(pos_pool))\n            rule_negatives[rule] = list(set(neg_pool))\n\n        for rule in test_df['rule'].unique():\n            clean_rule = cleaner(str(rule))\n            pos_pool = rule_positives[rule]\n            neg_pool = rule_negatives[rule]\n            n_samples = min(augmentation_factor * len(pos_pool), len(pos_pool) * len(neg_pool))\n            for _ in range(n_samples):\n                if pos_pool and neg_pool:\n                    anchors.append(clean_rule)\n                    positives.append(random.choice(pos_pool))\n                    negatives.append(random.choice(neg_pool))\n\n    combined = list(zip(anchors, positives, negatives))\n    random.shuffle(combined)\n    original_count = len(combined)\n    if subsample_fraction < 1.0:\n        n_samples = int(len(combined) * subsample_fraction)\n        combined = combined[:n_samples]\n        print(f\"Subsampled {original_count} -> {len(combined)} triplets ({subsample_fraction*100:.1f}%)\")\n    anchors, positives, negatives = zip(*combined) if combined else ([], [], [])\n    print(f\"Created {len(anchors)} triplets from test data\")\n    dataset = Dataset.from_dict({'anchor': list(anchors), 'positive': list(positives), 'negative': list(negatives)})\n    return dataset\n\n\ndef fine_tune_model(model, train_dataset, epochs=3, batch_size=32, learning_rate=2e-5, margin=0.25, output_dir=\"./models/test-finetuned-bge\"):\n    \"\"\"Fine-tune the sentence transformer model using triplet loss on test data.\"\"\"\n    print(f\"Fine-tuning model on {len(train_dataset)} triplets...\")\n    loss = TripletLoss(model=model, triplet_margin=margin)\n    dataset_size = len(train_dataset)\n    steps_per_epoch = max(1, dataset_size // batch_size)\n    max_steps = steps_per_epoch * epochs\n    args = SentenceTransformerTrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=epochs,\n        per_device_train_batch_size=batch_size,\n        warmup_steps=0,\n        learning_rate=learning_rate,\n        logging_steps=max(1, max_steps // 4),\n        save_strategy=\"epoch\",\n        save_total_limit=1,\n        fp16=True,  # 원본 유지(=GPU 환경 가정)\n        max_grad_norm=1.0,\n        dataloader_drop_last=False,\n        gradient_checkpointing=True,\n        gradient_accumulation_steps=1,\n        max_steps=max_steps,\n        report_to=\"none\"\n    )\n    trainer = SentenceTransformerTrainer(model=model, args=args, train_dataset=train_dataset, loss=loss)\n    trainer.train()\n    final_model_path = f\"{output_dir}/final\"\n    print(f\"Saving fine-tuned model to {final_model_path}...\")\n    model.save_pretrained(final_model_path)\n    return model, final_model_path\n\n\ndef load_or_create_finetuned_model(test_df):\n    \"\"\"Load fine-tuned model if exists, otherwise create and fine-tune it.\"\"\"\n    fine_tuned_path = \"./models/test-finetuned-bge/final\"\n    if os.path.exists(fine_tuned_path):\n        print(f\"Loading existing fine-tuned model from {fine_tuned_path}...\")\n        try:\n            word_embedding_model = models.Transformer(fine_tuned_path, max_seq_length=128, do_lower_case=True)\n            pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n            model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n            print(\"Loaded fine-tuned model with explicit pooling\")\n        except:\n            model = SentenceTransformer(fine_tuned_path)\n            print(\"Loaded fine-tuned model with default configuration\")\n        model.half()  # 원본과 동일하게 half()\n        return model\n\n    print(\"Fine-tuned model not found. Creating new one...\")\n    print(\"Loading base BGE embedding model...\")\n    try:\n        model_path = \"/kaggle/input/baai/transformers/bge-base-en-v1.5/1\"\n        word_embedding_model = models.Transformer(model_path, max_seq_length=256, do_lower_case=True)\n        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n        base_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n        print(\"Loaded base model from Kaggle path with explicit pooling\")\n    except:\n        model_path = \"\"  # BAAI/bge-small-en-v1.5\n        word_embedding_model = models.Transformer(model_path, max_seq_length=256, do_lower_case=True)\n        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n        base_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n        print(\"Loaded base model from local path with explicit pooling\")\n\n    triplet_dataset = create_test_triplet_dataset(test_df, augmentation_factor=2, subsample_fraction=1.)\n    fine_tuned_model, model_path = fine_tune_model(\n        model=base_model,\n        train_dataset=triplet_dataset,\n        epochs=1,\n        batch_size=16,\n        learning_rate=2e-5,\n        margin=0.25\n    )\n    print(f\"Fine-tuning completed. Model saved to: {model_path}\")\n    fine_tuned_model.half()  # 원본과 동일\n    return fine_tuned_model\n\n\ndef generate_rule_embeddings(test_df, model):\n    \"\"\"Generate embeddings for each unique rule.\"\"\"\n    print(\"Generating rule embeddings...\")\n    unique_rules = test_df['rule'].unique()\n    rule_embeddings = {}\n    for rule in unique_rules:\n        clean_rule = cleaner(str(rule))\n        rule_emb = model.encode(clean_rule, convert_to_tensor=False, normalize_embeddings=True)\n        rule_embeddings[rule] = rule_emb\n    print(f\"Generated embeddings for {len(rule_embeddings)} rules\")\n    return rule_embeddings\n\n\n# -----------------------------\n# 핵심 수정: UMAP crash 회피(최소 변경)\n#   - 원본 조건(샘플 수 > 10 그리고 > 32) 유지\n#   - 실제 UMAP 호출 시, n_components는 min(32, N-2)로만 clamp\n#   - spectral init(기본값) 유지 → 원본과 가장 가깝게\n# -----------------------------\ndef create_rule_centroids_with_hierarchical_clustering(test_df, text_to_embedding, rule_embeddings):\n    \"\"\"Create centroids using Hierarchical Clustering + UMAP for better cluster representation.\"\"\"\n    print(f\"\\nCreating rule centroids with Hierarchical Clustering + UMAP...\")\n    base_umap_components = 32\n    rule_centroids = {}\n\n    for rule in test_df['rule'].unique():\n        rule_data = test_df[test_df['rule'] == rule]\n\n        pos_embeddings = []\n        for _, row in rule_data.iterrows():\n            for col in ['positive_example_1', 'positive_example_2']:\n                if pd.notna(row[col]):\n                    clean_text = cleaner(str(row[col]))\n                    if clean_text in text_to_embedding:\n                        pos_embeddings.append(text_to_embedding[clean_text])\n\n        neg_embeddings = []\n        for _, row in rule_data.iterrows():\n            for col in ['negative_example_1', 'negative_example_2']:\n                if pd.notna(row[col]):\n                    clean_text = cleaner(str(row[col]))\n                    if clean_text in text_to_embedding:\n                        neg_embeddings.append(text_to_embedding[clean_text])\n\n        if pos_embeddings and neg_embeddings:\n            pos_embeddings = np.array(pos_embeddings)\n            neg_embeddings = np.array(neg_embeddings)\n\n            # --- 원본 조건 유지: n > 10 and n > 32 일 때만 UMAP ---\n            # 단, 호출 직전에 n_components를 N-2로 clamp하여 k>=N 에러 회피\n            def maybe_umap(X):\n                n = X.shape[0]\n                if n > 10 and n > base_umap_components:\n                    n_components_safe = min(base_umap_components, max(2, n - 2))\n                    reducer = UMAP(n_components=n_components_safe, random_state=42)  # init='spectral'(default)\n                    return reducer.fit_transform(X)\n                else:\n                    return X\n\n            pos_reduced = maybe_umap(pos_embeddings)\n            neg_reduced = maybe_umap(neg_embeddings)\n\n            # Agglomerative clustering (원본 동일)\n            n_pos_clusters = min(3, len(pos_embeddings))\n            n_neg_clusters = min(3, len(neg_embeddings))\n\n            pos_centroids = []\n            neg_centroids = []\n\n            if n_pos_clusters > 1:\n                pos_clusterer = AgglomerativeClustering(n_clusters=n_pos_clusters)\n                pos_labels = pos_clusterer.fit_predict(pos_reduced)\n                for cluster_id in np.unique(pos_labels):\n                    cluster_mask = pos_labels == cluster_id\n                    cluster_embeddings = pos_embeddings[cluster_mask]\n                    cluster_centroid = cluster_embeddings.mean(axis=0)\n                    cluster_centroid = cluster_centroid / np.linalg.norm(cluster_centroid)\n                    pos_centroids.append(cluster_centroid)\n            else:\n                pos_centroid = pos_embeddings.mean(axis=0)\n                pos_centroid = pos_centroid / np.linalg.norm(pos_centroid)\n                pos_centroids.append(pos_centroid)\n\n            if n_neg_clusters > 1:\n                neg_clusterer = AgglomerativeClustering(n_clusters=n_neg_clusters)\n                neg_labels = neg_clusterer.fit_predict(neg_reduced)\n                for cluster_id in np.unique(neg_labels):\n                    cluster_mask = neg_labels == cluster_id\n                    cluster_embeddings = neg_embeddings[cluster_mask]\n                    cluster_centroid = cluster_embeddings.mean(axis=0)\n                    cluster_centroid = cluster_centroid / np.linalg.norm(cluster_centroid)\n                    neg_centroids.append(cluster_centroid)\n            else:\n                neg_centroid = neg_embeddings.mean(axis=0)\n                neg_centroid = neg_centroid / np.linalg.norm(neg_centroid)\n                neg_centroids.append(neg_centroid)\n\n            rule_centroids[rule] = {\n                'positive_centroids': pos_centroids,\n                'negative_centroids': neg_centroids,\n                'pos_count': len(pos_embeddings),\n                'neg_count': len(neg_embeddings),\n                'rule_embedding': rule_embeddings[rule]\n            }\n\n            print(f\"  Rule: {rule[:50]}... - Pos: {len(pos_embeddings)}, Neg: {len(neg_embeddings)} - Clusters: Pos={len(pos_centroids)}, Neg={len(neg_centroids)}\")\n\n    print(f\"Created hierarchical centroids for {len(rule_centroids)} rules\")\n    return rule_centroids\n\n\ndef predict_test_set_with_hierarchical_clustering(test_df, text_to_embedding, rule_centroids):\n    \"\"\"Predict test set using hierarchical clustering centroids and distance metrics.\"\"\"\n    print(\"\\nMaking predictions on test set with Hierarchical Clustering centroids...\")\n    row_ids = []\n    predictions = []\n    for rule in test_df['rule'].unique():\n        print(f\"  Processing rule: {rule[:50]}...\")\n        rule_data = test_df[test_df['rule'] == rule]\n        if rule not in rule_centroids:\n            continue\n        pos_centroids = rule_centroids[rule]['positive_centroids']\n        neg_centroids = rule_centroids[rule]['negative_centroids']\n        for _, row in rule_data.iterrows():\n            body = cleaner(str(row['body']))\n            row_id = row['row_id']\n            if body not in text_to_embedding:\n                continue\n            body_embedding = text_to_embedding[body]\n            pos_distances = []\n            for pos_centroid in pos_centroids:\n                distance = np.linalg.norm(body_embedding - pos_centroid)\n                pos_distances.append(distance)\n            neg_distances = []\n            for neg_centroid in neg_centroids:\n                distance = np.linalg.norm(body_embedding - neg_centroid)\n                neg_distances.append(distance)\n            min_pos_distance = min(pos_distances) if pos_distances else 1.0\n            min_neg_distance = min(neg_distances) if neg_distances else 1.0\n            rule_prediction = min_neg_distance - min_pos_distance\n            row_ids.append(row_id)\n            predictions.append(rule_prediction)\n    print(f\"Made predictions for {len(predictions)} test examples\")\n    return row_ids, np.array(predictions)\n\n\ndef main():\n    print(\"=\"*70)\n    print(\"IMPROVED SIMILARITY CLASSIFIER - HIERARCHICAL CLUSTERING + UMAP (same results as your script)\")\n    print(\"=\"*70)\n\n    test_df = load_test_data()\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"MODEL PREPARATION PHASE\")\n    print(\"=\"*50)\n    model = load_or_create_finetuned_model(test_df)\n\n    all_texts = collect_all_texts(test_df)\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"EMBEDDING GENERATION PHASE\")\n    print(\"=\"*50)\n    all_embeddings = generate_embeddings(all_texts, model)\n\n    text_to_embedding = {text: emb for text, emb in zip(all_texts, all_embeddings)}\n\n    rule_embeddings = generate_rule_embeddings(test_df, model)\n\n    # 동일 로직 + UMAP crash만 회피\n    rule_centroids = create_rule_centroids_with_hierarchical_clustering(test_df, text_to_embedding, rule_embeddings)\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"PREDICTION PHASE\")\n    print(\"=\"*50)\n    row_ids, predictions = predict_test_set_with_hierarchical_clustering(test_df, text_to_embedding, rule_centroids)\n\n    submission_df = pd.DataFrame({'row_id': row_ids, 'rule_violation': predictions})\n    submission_df.to_csv('Triplet_submission.csv', index=False)\n\n    print(f\"\\nSaved predictions for {len(submission_df)} test examples to submission.csv and Triplet_submission.csv\")\n\n    print(f\"\\n{'='*70}\")\n    print(f\"HIERARCHICAL CLUSTERING + UMAP INFERENCE COMPLETED\")\n    print(f\"Model: Fine-tuned BGE on test data triplets\")\n    print(f\"Method: Hierarchical clustering + UMAP dimensionality reduction\")\n    print(f\"Predicted on {len(test_df)} test examples\")\n    print(f\"Prediction stats: min={predictions.min():.4f}, max={predictions.max():.4f}, mean={predictions.mean():.4f}\")\n    print(f\"{'='*70}\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T00:05:18.352649Z","iopub.execute_input":"2025-10-15T00:05:18.352883Z","iopub.status.idle":"2025-10-15T00:05:18.365511Z","shell.execute_reply.started":"2025-10-15T00:05:18.352858Z","shell.execute_reply":"2025-10-15T00:05:18.364887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile deberta.py\n# ============================================================\n# DeBERTa Multi-Seed Train + Rank-Average Ensemble (Single File)\n# Run with:  !python deberta.py\n# → writes /kaggle/working/deberta_submission.csv by default\n# ============================================================\n\nimport os\nos.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\nos.environ.setdefault(\"HF_HUB_DISABLE_TELEMETRY\", \"1\")\nos.environ.setdefault(\"TRANSFORMERS_NO_TORCHVISION\", \"1\")\n\nimport re\nimport argparse\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom typing import List\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments\n)\n\n# =========================\n# URL semantics extraction\n# =========================\nURL_PATTERN = re.compile(r'https?://[^\\s/$.?#].[^\\s]*', re.IGNORECASE)\nDOMAIN_PATTERN = re.compile(r\"(?:https?://)?([a-z0-9\\-\\.]+)\\.[a-z]{2,}\", re.IGNORECASE)\nCLEAN_PATH_PART_PATTERN = re.compile(r\"\\.(html?|php|asp|jsp)$|#.*|\\?.*\", re.IGNORECASE)\n\ndef url_to_semantics(text: str) -> str:\n    if not isinstance(text, str):\n        return \"\"\n    urls = URL_PATTERN.findall(text)\n    if not urls:\n        return \"\"\n    all_semantics, seen = [], set()\n    for url in urls:\n        u = url.lower()\n        m = DOMAIN_PATTERN.search(u)\n        if m:\n            for part in m.group(1).split('.'):\n                if part and len(part) > 3 and part not in seen:\n                    all_semantics.append(f\"domain:{part}\")\n                    seen.add(part)\n        path = re.sub(r\"^(?:https?://)?[a-z0-9\\.-]+\\.[a-z]{2,}/?\", \"\", u)\n        for p in [p for p in re.split(r'[/_.-]+', path) if p and p.isalnum()]:\n            pc = CLEAN_PATH_PART_PATTERN.sub(\"\", p)\n            if pc and len(pc) > 3 and pc not in seen:\n                all_semantics.append(f\"path:{pc}\")\n                seen.add(pc)\n    return (\"\\nURL Keywords: \" + \" \".join(all_semantics)) if all_semantics else \"\"\n\n# =========================\n# Data assembly\n# =========================\ndef _extract_violation_examples(df: pd.DataFrame) -> List[pd.DataFrame]:\n    out = []\n    for k in (\"positive\", \"negative\"):\n        label = 1 if k == \"positive\" else 0\n        for i in (1, 2):\n            col = f\"{k}_example_{i}\"\n            if col in df.columns:\n                sub = df[[col, \"rule\", \"subreddit\"]].copy()\n                sub.rename(columns={col: \"body\"}, inplace=True)\n                sub[\"rule_violation\"] = label\n                sub.dropna(subset=[\"body\"], inplace=True)\n                sub = sub[sub[\"body\"].str.strip().str.len() > 0]\n                if not sub.empty:\n                    out.append(sub)\n    return out\n\ndef get_dataframe_to_train(data_path: str, seed: int = 42) -> pd.DataFrame:\n    train_df = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n    test_df  = pd.read_csv(os.path.join(data_path, \"test.csv\"))\n    combined = []\n    if {\"body\",\"rule\",\"subreddit\",\"rule_violation\"}.issubset(train_df.columns):\n        combined.append(train_df[[\"body\",\"rule\",\"subreddit\",\"rule_violation\"]].copy())\n    combined.extend(_extract_violation_examples(train_df))\n    combined.extend(_extract_violation_examples(test_df))\n    if not combined:\n        return pd.DataFrame(columns=[\"body\",\"rule\",\"subreddit\",\"rule_violation\"])\n    full_df = pd.concat(combined, ignore_index=True)\n    full_df.drop_duplicates(subset=[\"body\",\"rule\",\"subreddit\"], inplace=True)\n    full_df.drop_duplicates(subset=[\"body\",\"rule\"], keep=\"first\", inplace=True)\n    return full_df.sample(frac=1, random_state=seed).reset_index(drop=True)\n\n# =========================\n# Reproducibility\n# =========================\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# =========================\n# Dataset\n# =========================\nclass JigsawDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n    def __getitem__(self, idx):\n        item = {k: torch.tensor(v[idx]) for k,v in self.encodings.items()}\n        if self.labels is not None:\n            item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n\n# =========================\n# Build inputs\n# =========================\ndef add_url_semantics_column(series_body: pd.Series) -> pd.Series:\n    return series_body.apply(lambda x: (x if isinstance(x, str) else \"\") + url_to_semantics(x))\n\ndef build_input_text(df: pd.DataFrame) -> pd.Series:\n    return df[\"rule\"].astype(str) + \"[SEP]\" + df[\"body_with_url\"].astype(str)\n\ndef prepare_dataset(df: pd.DataFrame, tokenizer, max_length: int, is_train: bool) -> JigsawDataset:\n    enc = tokenizer(df[\"input_text\"].tolist(), truncation=True, padding=True, max_length=max_length)\n    return JigsawDataset(enc, df[\"rule_violation\"].tolist() if is_train else None)\n\n# =========================\n# Train & Predict (one seed)\n# =========================\ndef train_and_predict_one_seed(\n    seed: int,\n    model_name_or_path: str,\n    data_path: str,\n    output_dir: str,\n    epochs: int = 2,\n    lr: float = 2e-5,\n    max_length: int = 512,\n    batch_size: int = 8,\n    save_each_submission: bool = True\n) -> pd.DataFrame:\n    seed_everything(seed)\n    train_df = get_dataframe_to_train(data_path, seed=seed)\n    train_df = train_df.copy()\n    train_df[\"body_with_url\"] = add_url_semantics_column(train_df[\"body\"])\n    train_df[\"input_text\"] = build_input_text(train_df)\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n    train_ds = prepare_dataset(train_df, tokenizer, max_length, is_train=True)\n\n    model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, num_labels=2)\n    training_args = TrainingArguments(\n        output_dir=os.path.join(output_dir, f\"s{seed}\"),\n        num_train_epochs=epochs,\n        learning_rate=lr,\n        per_device_train_batch_size=batch_size,\n        warmup_ratio=0.1,\n        weight_decay=0.01,\n        report_to=\"none\",\n        save_strategy=\"no\",\n        logging_steps=10,\n        dataloader_num_workers=2,\n        fp16=torch.cuda.is_available(),\n    )\n    trainer = Trainer(model=model, args=training_args, train_dataset=train_ds)\n    trainer.train()\n\n    test_df = pd.read_csv(os.path.join(data_path, \"test.csv\")).copy()\n    test_df[\"body_with_url\"] = add_url_semantics_column(test_df[\"body\"])\n    test_df[\"input_text\"] = build_input_text(test_df)\n    test_ds = prepare_dataset(test_df, tokenizer, max_length, is_train=False)\n\n    out = trainer.predict(test_ds)\n    probs = torch.nn.functional.softmax(torch.tensor(out.predictions), dim=1)[:, 1].numpy()\n\n    sub = pd.DataFrame({\"row_id\": test_df[\"row_id\"].values, \"rule_violation\": probs})\n    if save_each_submission:\n        p = os.path.join(\"/kaggle/working\", f\"submission_s{seed}.csv\")\n        sub.to_csv(p, index=False)\n    return sub\n\n# =========================\n# Rank-average ensemble\n# =========================\ndef rank_average_ensemble(subs: List[pd.DataFrame]) -> pd.DataFrame:\n    base = subs[0][[\"row_id\"]].drop_duplicates().copy()\n    cols = []\n    for i, df in enumerate(subs):\n        c = f\"s{i}\"\n        df_ = df[[\"row_id\",\"rule_violation\"]].drop_duplicates(\"row_id\").rename(columns={\"rule_violation\": c})\n        base = base.merge(df_, on=\"row_id\", how=\"inner\")\n        cols.append(c)\n    n = len(base)\n    for c in cols:\n        s = pd.to_numeric(base[c], errors=\"coerce\").fillna(base[c].median())\n        base[c] = s.rank(method=\"average\") / (n + 1)\n    base[\"rule_violation\"] = base[cols].mean(axis=1)\n    return base[[\"row_id\",\"rule_violation\"]].sort_values(\"row_id\").reset_index(drop=True)\n\n# =========================\n# Defaults & CLI\n# =========================\ndef resolve_model_path(user_path: str) -> str:\n    for p in [user_path,\n              \"/kaggle/input/debertav3base\",\n              \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-base\",\n              \"/kaggle/input/deberta-v3-small/deberta-v3-small\"]:\n        if p and os.path.exists(p):\n            return p\n    return user_path\n\ndef parse_args():\n    ap = argparse.ArgumentParser(description=\"DeBERTa multi-seed training + rank-average ensemble (single file).\",\n                                 add_help=True)\n    ap.add_argument(\"--seeds\", type=str, default=\"42,43,44\",\n                    help=\"Comma-separated seeds (default: 42,43,44)\")\n    ap.add_argument(\"--model-path\", type=str, default=\"/kaggle/input/debertav3base\",\n                    help=\"Local model path (offline).\")\n    ap.add_argument(\"--data-path\", type=str, default=\"/kaggle/input/jigsaw-agile-community-rules/\",\n                    help=\"Path containing train.csv and test.csv\")\n    ap.add_argument(\"--output-dir\", type=str, default=\"./dbv3_base_ens_model\",\n                    help=\"Directory for trainer outputs\")\n    ap.add_argument(\"--epochs\", type=int, default=2)\n    ap.add_argument(\"--batch-size\", type=int, default=8)\n    ap.add_argument(\"--max-length\", type=int, default=512)\n    ap.add_argument(\"--lr\", type=float, default=2e-5)\n    ap.add_argument(\"--final-outfile\", type=str, default=\"/kaggle/working/deberta_submission.csv\",\n                    help=\"Final ensemble path\")\n    ap.add_argument(\"--skip-save-per-seed\", action=\"store_true\")\n    return ap.parse_args()\n\ndef main():\n    args = parse_args()  # all defaults chosen so `!python deberta.py` just works\n    seeds = [int(s) for s in args.seeds.split(\",\") if s.strip().isdigit()]\n    model_path = resolve_model_path(args.model_path)\n    os.makedirs(args.output_dir, exist_ok=True)\n\n    subs = []\n    for seed in seeds:\n        subs.append(\n            train_and_predict_one_seed(\n                seed=seed,\n                model_name_or_path=model_path,\n                data_path=args.data_path,\n                output_dir=args.output_dir,\n                epochs=args.epochs,\n                lr=args.lr,\n                max_length=args.max_length,\n                batch_size=args.batch_size,\n                save_each_submission=not args.skip_save_per_seed\n            )\n        )\n\n    final_sub = rank_average_ensemble(subs)\n    final_sub.to_csv(args.final_outfile, index=False)\n    print(final_sub.head())\n    print(f\"[OK] Wrote final ensemble: {args.final_outfile} (rows={len(final_sub)})\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T00:05:18.366314Z","iopub.execute_input":"2025-10-15T00:05:18.366537Z","iopub.status.idle":"2025-10-15T00:05:18.382401Z","shell.execute_reply.started":"2025-10-15T00:05:18.366515Z","shell.execute_reply":"2025-10-15T00:05:18.381716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python triplet.py\n%env RUN_MODE=train\n!python deberta.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T00:05:18.383108Z","iopub.execute_input":"2025-10-15T00:05:18.383424Z","iopub.status.idle":"2025-10-15T00:08:06.704684Z","shell.execute_reply.started":"2025-10-15T00:05:18.383406Z","shell.execute_reply":"2025-10-15T00:08:06.703759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# ---------- 1) 파일 읽기 & 키 이름 통일 ----------\ndeb = pd.read_csv(\"deberta_submission.csv\")[[\"row_id\", \"rule_violation\"]].rename(columns={\"rule_violation\":\"deb\"})\ntri = pd.read_csv(\"Triplet_submission.csv\")[[\"row_id\", \"rule_violation\"]].rename(columns={\"rule_violation\":\"tri\"})\nq   = pd.read_csv(\"submission_qwen.csv\")[[\"row_id\", \"rule_violation\"]].rename(columns={\"rule_violation\":\"q\"})\nq3  = pd.read_csv(\"submission_qwen3.csv\")[[\"row_id\", \"rule_violation\"]].rename(columns={\"rule_violation\":\"q3\"})\n\n# 교집합 머지(순서 보장 위해 row_id 기준 inner join)\ndf = deb.merge(tri, on=\"row_id\", how=\"inner\").merge(q, on=\"row_id\", how=\"inner\").merge(q3, on=\"row_id\", how=\"inner\")\nmodels = [\"deb\", \"tri\", \"q\", \"q3\"]\n\n# ---------- 2) [0,1] 랭크 정규화 ----------\ndef to_rank01(s: pd.Series) -> pd.Series:\n    s = pd.to_numeric(s, errors=\"coerce\")\n    return s.rank(method=\"average\") / (len(s) + 1)\n\nfor m in models:\n    df[f\"{m}_r\"] = to_rank01(df[m])\n\nrcols = [f\"{m}_r\" for m in models]\n\n# ---------- 3) 분산(ensemble 평균 대비) 계산 ----------\nrmean = df[rcols].mean(axis=1)\nvariances = {}\nfor m in models:\n    variances[m] = float(((df[f\"{m}_r\"] - rmean) ** 2).mean())\n\n# ---------- 4) 분산 역수 가중치 + PRIOR (q, tri 우대) ----------\nEPS = 1e-12\nbase_w = {m: 1.0 / (variances[m] + EPS) for m in models}\n\n# 원하는 우선순위(필요시 미세조정): q 가장 높게, 그 다음 tri\nPRIORS = {\n    \"deb\": 1.4,\n    \"tri\": 1.3,  # ↑ triplet 가중 우대\n    \"q\"  : 1.6,  # ↑ qwen 가중 최우대\n    \"q3\" : 0.5,  # 보조\n}\n\nw = {m: base_w[m] * PRIORS[m] for m in models}\nwsum = sum(w.values())\nw = {m: w[m] / wsum for m in models}\n\nprint(\"Model variances (lower is better):\", variances)\nprint(\"Final weights:\", w)\n\n# ---------- 5) 가중 랭크 블렌딩 ----------\ndf[\"rule_violation\"] = sum(w[m] * df[f\"{m}_r\"] for m in models)\n\n# ---------- 6) 저장 ----------\nout = df[[\"row_id\", \"rule_violation\"]].copy()\nout = out.sort_values(\"row_id\").reset_index(drop=True)\nout.to_csv(\"/kaggle/working/submission.csv\", index=False)\nprint(f\"Saved -> /kaggle/working/submission.csv  (rows={len(out)})\")\n\n# 참고: 스케일 후 상관관계\nprint(\"Rank corr:\\n\", df[rcols].corr())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T00:08:06.706008Z","iopub.execute_input":"2025-10-15T00:08:06.706332Z","iopub.status.idle":"2025-10-15T00:08:06.815608Z","shell.execute_reply.started":"2025-10-15T00:08:06.706296Z","shell.execute_reply":"2025-10-15T00:08:06.814664Z"}},"outputs":[],"execution_count":null}]}